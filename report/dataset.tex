\section{Data}
Our project takes an empirical approach to the algorithm selection problem.
Thus, our first step is to collect data for analysis.

\subsection{Generating Data}
We featurize matrix multiplications based on the dimensions of the input matrices: M, K, and N.
These features represent the size of each dimension of a matrix multiplication of the form $M\times{K} * K\times{N}$.
We generated random dense matrices with real-valued floating point numbers ranging in size from $64\times{64}$ to $3250\times{3250}$.
We varied each dimension in evenly-spaced increments across that range.\shaddi{David, you should add more detail about this}.
Note that this means we generated a range of rectangular matrices, not just square ones.

This step results in 1000 possible matrix multiplications (i.e., combinations of M, K, and N).
We then run each of these matrix multiplications using both of our algorithms under test (MKL and CARMA) and record the performance of each multiplication in gigaflops/second.
We repeat each multiplication three times and compare the mean performance for each algorithm.
This constitutes a single datapoint for our classifier: a tuple (M,K,N) and a binary label of which algorithm performed better.
Finally, we performed this process on three separate machines with different architectures, for a total of 18,000 matrix multiplications and 3,000 data points.
The machines we used are shown in Table~\ref{t:machines}.

\shaddi{Maybe say something about the tools we used to generate and run the experiments?}

\begin{table}[t]
    \begin{center}
        \begin{tabular}{c|c|c|c}
            Machine & Cores & CPU Type & Memory \\ \hline
            Emerald & 32 & Intel Xeon X7560 & 128GB \\
            Hopper & 24 & AMD ``MagnyCours'' & 32GB \\
            Sandy & 8 & Intel i7-2600 & 16GB \\
        \end{tabular}
    \end{center}
    \caption{Machines used for evaluation.}
    \label{t:machines}
\end{table}

\subsection{Limitations}
Our data has a few limitations.
First and foremost, our largest matrix is $3250\times{3250}$.
While this is a large matrix, it still fit in memory on all our machines; we did not explore extremely large matrix sizes, though we believe our technique is general enough to scale to those as well.
Secondly, some of the machines we used to generate data were shared machines.
As a result, our performance would sometimes fluctuate significantly between runs; we considered average performance to account for this, but due to time constraints we were only able to consider an average of three runs.
Nevertheless, our classification results were strong, so we do not believe that the noise introduced by this significantly impacted our evaluation.
